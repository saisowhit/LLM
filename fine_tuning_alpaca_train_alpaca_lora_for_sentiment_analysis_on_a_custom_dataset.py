# -*- coding: utf-8 -*-
"""Fine-tuning Alpaca: Train Alpaca LoRa for Sentiment Analysis on a Custom Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XU5FZFm4VWXK-cXaGcB57k6QIOjF7lqU
"""

!pip3 install -U pip
!pip3 install accelerate==0.18.0
!pip3 install fire==0.5.0
!pip3 install torch==2.0.0
!pip3 install gradio==3.23.0
!pip3 install tensorboardX==2.6
!pip3 install datasets==2.10.1
!pip3 install bitsandbytes==0.37.2
!pip3 install appdirs==1.4.4
!pip3 install git+https://github.com/huggingface/peft/git
!pip3 install git+https://github.com/huggingface/transformers/tree/main#egg=transformers
!pip3 install sentencepiece

import fire
import json
import textwrap
import torch
import pandas as pd
import seaborn as sns
import os
import sys
from peft import (LoraConfig,get_peft_model,prepare_model_for_int8_training)
import matplotlib.pyplt as plt

device="cuda" if torch.cuda.is_available() else "cpu"

# !gdown !1xQ89cpZCnafsW5t3G3ZQWvR7q682t2BN

df=pd.read_csv("bitcoin-sentiment-tweets.csv")

def sentiment_score_to_name(score):
  if score>0:
    return "positive"
  elif score<0:
    return "negative"
  else:
    return "neutral"

dataset_data=[
    {"instruction":"Detect sentiment to the tweet","input":row_dict['tweet'],'output':sentiment_score_to_name(row_dict['sentiment'])} for row_dict in df.to_dict(orient='records')
]

dataset_data[0]

with open("alpaca-bitcoin-sentiment-dataset.json","w") as f:
  json.dump(dataset_data,f)

BASE_MODEL="decapoda-resarch/llama-7b-hf"

model=LlamaForCasualLM.from_pretained(BASE_MODEL,load_in_bit=True,torch_dtype=torch.float32,device_map="auto")
tokenizer=LlamaTokenizer.from_pretrained(BASE_MODEL)
tokenizer.pad_token_id=(0)
tokenizer.pad_side='left'
data=load_dataset("json",data_files="alpaca-bitcoin-sentiment-dataset.json")
data['train']
#

deg generate_prompt(data_point):
return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
{data_point['instruction']}
### Input:
{data_point['input']}
### Response:

def tokenize(prompt,add_eos_token=True):
result=tokenizer(prompt,truncation=True,padding=False,return_tensors=None)
if(add_eos_token):
result['input_ids'].append(tokenizer.eos)
return result

model=prepare_model_for_int8_training(model)
config=LoraConfig(r=8,lora_alpha=32,target_modules=["q_proj","v_proj"],lora_dropout=0.05,bias="none",task_type="CAUSAL_LM")
model=get_peft_model(model,config)
model.print_trainable_parameters

def generate_and_tokenize_prompt(data_point):
  full_prompt=generate_prompt(data_point)
  tokenized_full_prompt=tokenize(full_prompt)
  return tokenized_full_prompt

train_val=data['train'].train_test_split(test_size=0.2,shuffle=True,seed=42)
train_data=train_val['train'].shuffle().map(generate_and_tokenize_prompt)
eval_data=train_val['test'].shuffle().map(generate_and_tokenize_prompt)

val_data=train_val['test'].shuffle().map(generate_and_tokenize_prompt)

LORA_R='lora-alpaca-bitcoin-sentiment'
LORA_ALPHA=16
LORA_DROPOUT=0.05
LORA_TARGET_MODULES=['q_proj','v_proj']
MICRO_BATCH_SIZE=4
BATCH_SIZE=128
GRADIENT_ACCUMULATION_STEPS=BATCH_SIZE//MICRO_BATCH_SIZE
LEARNING_RATE=3e-4
TRAIN_STEPS=100
output_dir=f'./{LORA}'

model=prepare_model_for_int8_training(model)
config=LoraConfig(r=8,lora_alpha=32,target_modules=["q_proj","v_proj"],lora_dropout=0.05,bias="none",task_type="CAUSAL_LM")
model=get_peft_model(model,config)

training_arguments=transformers.TrainingArguments(per_device_train_batch_size=MICRO_BATCH_SIZE,gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,warmup_steps=100,max_steps=TRAINING_STEPS,learning_rate=LEARNING_RATE,save_stratergy='STEPS',eval_steps=50,save_steps=50,output_dir=output_dir)
data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,mlm=False)
trainer=transformers.Trainer(model=model,train_dataset=data['train'],eval_dataset=data['test'],args=training_arguments,data_collator=data_collator)

model.config.use_cache=False
old_state_dict=model.state_dict
model.state_dict=(lambda self, *_, **__: get_peft_model_state_dict(self,old_state_dict())).__get__(model,type(model))
model=torch.compile(model)
trainer.train()
model.save_pretrained(new_model)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/tloen/alpaca-lora.git
# %cd alpaca-lora
!git checkout a48d947
!python generate.py \
--load 8bit\
--base_model "decapoda-research/llama-7b-hf"
--lora-weights "curiouslly/alpaca-bitcoin-tweets-sentiment"\
--share_gradio