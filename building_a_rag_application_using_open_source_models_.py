# -*- coding: utf-8 -*-
"""Building a RAG application using open-source models .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C6c-ZCb5YjB8bybDxJ-5rFyegGeBZG4i
"""

import os
import openai
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

model="gpt-3.5-turbo"
MODEL="mixtral:8x7b"
MODEL="llama2"

model=ChatOpenAI(api_key=OPENAI_API_KEY,model_name=MODEL, temperature=0)
model.invoke('Tell me a joke')

if model.startswith("llama"):
  model=Ollama(model=model, temperature=0)
else:
  model=ChatOpenAI(model=model, temperature=0)
model.invoke('Tell me a joke')

from langchain_core.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
parser=StrOutputParser()
chain=model | parser
chain.invoke(("Tell me a hilarious joke"))

from langchain_community.document_loader import PyPDFLoader
loader=PyPDFLoader("data/test.pdf")
pages=loader.load_and_split()
pages

from langchain.prompts import PromptTemplate
template="""
Answer the question based only on the following context. If you cant answer the question, just say you dont know.
{context}
Context: {context}

Question: {question}
"""

chain.input_schema.schema()

chain=prompt | model | parser
chain.invoke({"context":"my name is sai", "question":"what is your name"})

from langchain_community.vectorstores import DocArrayInMemorySearch
vectorstore=DocArrayInMemorySearch.from_pages(pages,embeddings=embeddings)
retriever=vectorstore.as_retriever()
retriever.invoke("what is my name",top_k=1)

chain=({"context":"Then name I was given was Sai","question":"what is my name"}|  prompt| model | parser)
chain.invoke({"context":"my name is sai", "question":"what is your name"})

itemgetter("abc")({"abc":123})

from operator import itemgetter
chain=({"context":"Then name I was given was Sai","question":"what is my name"}|  prompt| model | parser)
chain.invoke({"context":"my name is sai", "question":"what is your name"})
questions=["what is the purpose of the course","How may hours of live session"]
for question in questions:
  print(chain.invoke({"context":"my name is sai", "question":question}))
  print()

for s in chain.stream({"question":"what is the purpose of the course"}):
  print(s,end="",flush=True)

chain.batch([{"question":q} for q in question])